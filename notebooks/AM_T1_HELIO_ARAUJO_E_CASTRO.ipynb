{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As bibliotecas necessárias são importadas. A biblioteca Numpy para manipulação numérica, a biblioteca Pandas para criar os dataframes, os geradores de relatório Seaborn, os modelos de algoritmo de classificação pedidos no exercício: Regressão Logística, Arvore de Decisão, RandomForest, KNeighbors, Gradient Boosting, uma biblioteca para pré-processamento do dataset (StandardScaler), a biblioteca para medir a qualidade preditiva dos modelos (accuracy_score, confusion_matrix e classification_report) e a biblioteca para aplicar o Scaler para pré-processamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar as bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer a carga dos datasets a partir dos arquivos credtrain.txt e credtest.txt, passando o nome das colunas. Verificar se o dataset foi carregado corretamente e analisar o conteúdo do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o nomes das colunas\n",
    "nomes_colunas = ['ESCT', 'NDEP', 'RENDA', 'TIPOR', 'VBEM', 'NPARC', 'VPARC', 'TEL', 'IDADE', 'RESMS', 'ENTRADA', 'CLASSE']\n",
    "\n",
    "# Carrega o dataset de treinamento\n",
    "dsTrain = pd.read_csv('../data/credtrain.txt', names=nomes_colunas, sep='\\t', header=0)\n",
    "\n",
    "# Carrega o dataset de teste\n",
    "dsTeste = pd.read_csv('../data/credtest.txt', names=nomes_colunas, sep='\\t', header=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsTrain.head()\n",
    "dsTeste.head()\n",
    "dsTrain.info()\n",
    "dsTrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dataset apresentado é identificado alguns atributos categóricos, ainda que estejam representados numericamente, como: Estado Civil (ESCT), Tipo de residência (TIPOR) e se o cliente possui telefone (TEL). Nesses casos será aplicado a técnica de One-hot-encoding, que transpõe os domínios encontrados nestes atributos em campos nomeados destes valores. O campo de número de dependentes (NDEP), apesar de categório, apresenta-se de forma númerica e ordenada, característica coerente com a natureza deste atributo, sendo assim, não será necessário aplicar a técnica de One-hot-encoding em NDEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsTrain = pd.get_dummies(dsTrain, columns=['ESCT','TIPOR','TEL'])\n",
    "dsTeste = pd.get_dummies(dsTeste, columns=['ESCT','TIPOR','TEL'])\n",
    "\n",
    "dsTrain.head()\n",
    "dsTeste.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset é separado em features (X) e target (y) para o dataset de Treinamento (dsTrain) e de teste (dsTeste)\n",
    "\n",
    "Para equalizar os dados de entrada (feature), foi aplicado a função StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and the target variable (y)\n",
    "X_train = dsTrain.drop('CLASSE', axis=1)\n",
    "y_train = dsTrain['CLASSE']\n",
    "\n",
    "X_test = dsTeste.drop('CLASSE', axis=1)\n",
    "y_test = dsTeste['CLASSE']\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez realizado o pré-processamento dos dados, o modelo será treinado com os algoritmos solicitados. Verificando a acurácia de modelo.\n",
    "\n",
    "Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma lista para os valores de acurácia e os modelos\n",
    "listaModelos = ['Regressão Logística', 'Arvore de Decisão', 'Random Forest', 'K Neighbors', 'Gradient Boosting']\n",
    "listaAcuracia =[]\n",
    "\n",
    "# Aplicando a Regressão Logística\n",
    "modelLR = LogisticRegression(solver='lbfgs', random_state=0)\n",
    "modelLR.fit(X_train, y_train)\n",
    "\n",
    "# Fazer a predição da Regressão Logística\n",
    "y_predict_LR = modelLR.predict(X_test)\n",
    "\n",
    "\n",
    "# Gerar a matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_predict_LR)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = accuracy_score(y_test, y_predict_LR)\n",
    "listaAcuracia.append(accuracy)\n",
    "\n",
    "# Visualizar a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"BuGn\", cbar=False)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão (Regressão Logística)')\n",
    "plt.show()\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a Decision Tree Classifier\n",
    "modelTree = DecisionTreeClassifier(random_state=13)\n",
    "modelTree.fit(X_train, y_train)\n",
    "\n",
    "# Fazer a predição da Decision Tree Classifier\n",
    "y_predict_Tree = modelTree.predict(X_test)\n",
    "\n",
    "# Gerar a matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_predict_Tree)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = accuracy_score(y_test, y_predict_Tree)\n",
    "listaAcuracia.append(accuracy)\n",
    "\n",
    "# Visualizar a Decision Tree Classifier\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão (Decision Tree Classifier)')\n",
    "plt.show()\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a Random Forest Classifier\n",
    "modelRF = RandomForestClassifier(n_estimators=2, random_state=0)\n",
    "modelRF.fit(X_train, y_train)\n",
    "\n",
    "# Fazer a predição da Random Forest Classifier\n",
    "y_predict_RF = modelRF.predict(X_test)\n",
    "\n",
    "# Gerar a matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_predict_RF)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = accuracy_score(y_test, y_predict_RF)\n",
    "listaAcuracia.append(accuracy)\n",
    "\n",
    "# Visualizar a Random Forest Classifier\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão (Random Forest Classifier)')\n",
    "plt.show()\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a K Neighbors Classifier\n",
    "modelKN = KNeighborsClassifier(n_neighbors=5)\n",
    "modelKN.fit(X_train, y_train)\n",
    "\n",
    "# Fazer a predição de K Neighbors Classifier\n",
    "y_predict_KN = modelKN.predict(X_test)\n",
    "\n",
    "# Gerar a matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_predict_KN)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = accuracy_score(y_test, y_predict_KN)\n",
    "listaAcuracia.append(accuracy)\n",
    "\n",
    "# Visualizar a K Neighbors Classifier\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão (K Neighbor Classifier)')\n",
    "plt.show()\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a Gradient Boosting Classifier\n",
    "modelGB = GradientBoostingClassifier()\n",
    "modelGB.fit(X_train, y_train)\n",
    "\n",
    "# Fazer a predição de Gradient Boosting Classifier\n",
    "y_predict_GB = modelGB.predict(X_test)\n",
    "\n",
    "# Gerar a matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_predict_GB)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = accuracy_score(y_test, y_predict_GB)\n",
    "listaAcuracia.append(accuracy)\n",
    "\n",
    "# Visualizar a Gradiente Boosting Classifier\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão (Gradient Boosting Classifier)')\n",
    "plt.show()\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo com melhor precisão\n",
    "iMelhor=(listaAcuracia.index(max(listaAcuracia)))\n",
    "\n",
    "\n",
    "print(\"O melhor modelo é\", listaModelos[iMelhor], \"com acurácia de\", listaAcuracia[iMelhor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "Importar as bibliotecas necessárias (para efeito didático algumas bibliotecas serão carregadas novamente, as mesmas já carregadas no inicio do notebook) e carregar os dados em arquivo texto para o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar as bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar os dados dos diamantes a partir do arquivo de texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset a partir do arquivo texto\n",
    "dataset = pd.read_csv('../data/diamonds.csv', header=0)\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os campos cut, color e clarity são objetos e devem ser convertidos em valores numéricos para processamento dos algoritmos. Estes campos obedecem uma ordem qualitativa, então foi utilizado o recurso Ordinal Encoder para fazer esta transformação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.cut.unique()\n",
    "dataset.clarity.unique()\n",
    "\n",
    "# Determina um vetor com valores crescentes dos atributos cut, color e clarity\n",
    "order_cut = ['Fair','Good', 'Very Good', 'Premium', 'Ideal']\n",
    "order_color = ['J','I','H','G','F','E','D']\n",
    "order_clarity = ['I3','I2','I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF','FL']\n",
    "\n",
    "# Adiciona campo cut com numeraçao crescente\n",
    "order_encoder_cut=OrdinalEncoder(categories=[order_cut])\n",
    "cut_encoded = order_encoder_cut.fit_transform(dataset[['cut']])\n",
    "dataset['cut_encoded'] = cut_encoded\n",
    "\n",
    "# Adiciona campo color com numeraçao crescente\n",
    "order_encoder_color=OrdinalEncoder(categories=[order_color])\n",
    "color_encoded = order_encoder_color.fit_transform(dataset[['color']])\n",
    "dataset['color_encoded'] = color_encoded\n",
    "\n",
    "# Adiciona campo clarity com numeraçao crescente\n",
    "order_encoder_clarity=OrdinalEncoder(categories=[order_clarity])\n",
    "clarity_encoded = order_encoder_clarity.fit_transform(dataset[['clarity']])\n",
    "dataset['clarity_encoded'] = clarity_encoded\n",
    "\n",
    "# Remover as colunas de objetos que foram transformados\n",
    "dataset = dataset.drop('cut', axis=1)\n",
    "dataset = dataset.drop('color', axis=1)\n",
    "dataset = dataset.drop('clarity', axis=1)\n",
    "\n",
    "# Remove o campo identidade do ínico\n",
    "dataset = dataset.drop(dataset.columns[0], axis=1) \n",
    "dataset.head()\n",
    "dataset.drop(dataset.columns[0], axis=1)\n",
    "\n",
    "# Remover os registos onde a dimensão do diamante é zero\n",
    "dataset = dataset.drop(dataset[dataset[\"x\"]==0].index)\n",
    "dataset = dataset.drop(dataset[dataset[\"y\"]==0].index)\n",
    "dataset = dataset.drop(dataset[dataset[\"z\"]==0].index)\n",
    "\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E fazer a separação em dois datasets: treinamento e teste obedecendo a relação de 20% em relação ao total de registros. Para isto, será usado a biblioteca train_test_split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as features e o target do modelo (campo price) para predizer o valor\n",
    "X= dataset.drop([\"price\"],axis =1)\n",
    "y= dataset[\"price\"]\n",
    "\n",
    "# Separar em teste e treinamento na razão de 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar o treinamento com cada um dos algoritmos de regressão listados: Regressão Linear, Lasso, Decison Tree Regressor, K Neighbors Regressor e Gradient Boosting Regressor. Como esses algoritmos de regressão compartilham as mesmas chamadas de fit, predict, r2_score e mean_squared_error, utilizando os hiperparâmetros padrões, será definida uma lista com as funções a serem executadas em bloco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a lista dos algoritmos a serem utilizados\n",
    "listaAlgoritmosReg = [LinearRegression, Lasso, DecisionTreeRegressor, RandomForestRegressor, KNeighborsRegressor, GradientBoostingRegressor]\n",
    "\n",
    "# Armazena o melhor algoritmo com base no valor do r2 (maior = melhor)\n",
    "melhorR2 = 0\n",
    "melhorMse = 0\n",
    "melhorAlgoritmo = \"\"\n",
    "\n",
    "for modeloAlgo in listaAlgoritmosReg:\n",
    "    modeloSelecionado = modeloAlgo()\n",
    "    modeloSelecionado.fit(X_train, y_train)\n",
    "    y_pred_sel = modeloSelecionado.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred_sel)\n",
    "    r2 = r2_score(y_test, y_pred_sel)\n",
    "\n",
    "    if (r2 > melhorR2):\n",
    "        melhorR2 = r2\n",
    "        melhorMse= mse\n",
    "        melhorAlgoritmo = modeloAlgo.__name__\n",
    "\n",
    "\n",
    "    print(modeloAlgo.__name__)\n",
    "    print(\"MSE = \" ,mse)\n",
    "    print('R2 = %.2f' % r2)\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "print(\"Melhor algoritmo = \", melhorAlgoritmo)\n",
    "print(\"MSE = \", melhorMse)\n",
    "print(\"R2 = \", melhorR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "\n",
    "Como primeiro passo, vamos para a importação das bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer a importação dos dados a partir dos arquivos no formato pickle: A602.pickle, A621.pickle, A627.pickle, A636.pickle, A652.pickle. Depois unificar em um único dataset para análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/atmoseer/A602.pickle\n",
      "../data/atmoseer/A621.pickle\n",
      "../data/atmoseer/A627.pickle\n",
      "../data/atmoseer/A636.pickle\n",
      "../data/atmoseer/A652.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "      <td>49640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.421857</td>\n",
       "      <td>0.505501</td>\n",
       "      <td>0.747884</td>\n",
       "      <td>0.505829</td>\n",
       "      <td>0.471072</td>\n",
       "      <td>0.499833</td>\n",
       "      <td>0.495685</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.422227</td>\n",
       "      <td>0.505237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495875</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.422468</td>\n",
       "      <td>0.505024</td>\n",
       "      <td>0.746911</td>\n",
       "      <td>0.505727</td>\n",
       "      <td>0.471356</td>\n",
       "      <td>0.497698</td>\n",
       "      <td>0.496346</td>\n",
       "      <td>0.003880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.154925</td>\n",
       "      <td>0.160470</td>\n",
       "      <td>0.181923</td>\n",
       "      <td>0.131709</td>\n",
       "      <td>0.149439</td>\n",
       "      <td>0.353127</td>\n",
       "      <td>0.353960</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>0.155056</td>\n",
       "      <td>0.160375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353346</td>\n",
       "      <td>0.029044</td>\n",
       "      <td>0.155201</td>\n",
       "      <td>0.160298</td>\n",
       "      <td>0.182331</td>\n",
       "      <td>0.132072</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.354302</td>\n",
       "      <td>0.352784</td>\n",
       "      <td>0.029017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.395918</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.430825</td>\n",
       "      <td>0.353270</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.395918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.395918</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.430387</td>\n",
       "      <td>0.353372</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.403042</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.495847</td>\n",
       "      <td>0.459387</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403448</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403448</td>\n",
       "      <td>0.497976</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.495806</td>\n",
       "      <td>0.459387</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.513793</td>\n",
       "      <td>0.607287</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.571563</td>\n",
       "      <td>0.572553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516729</td>\n",
       "      <td>0.607287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.605691</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.571992</td>\n",
       "      <td>0.572898</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  49640.000000  49640.000000  49640.000000  49640.000000  49640.000000   \n",
       "mean       0.421857      0.505501      0.747884      0.505829      0.471072   \n",
       "std        0.154925      0.160470      0.181923      0.131709      0.149439   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.313433      0.395918      0.638889      0.430825      0.353270   \n",
       "50%        0.403042      0.500000      0.782051      0.495847      0.459387   \n",
       "75%        0.513793      0.607287      0.888889      0.571563      0.572553   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  49640.000000  49640.000000  49640.000000  49640.000000  49640.000000   \n",
       "mean       0.499833      0.495685      0.003897      0.422227      0.505237   \n",
       "std        0.353127      0.353960      0.029061      0.155056      0.160375   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.146447      0.146447      0.000000      0.313433      0.395918   \n",
       "50%        0.500000      0.500000      0.000000      0.403448      0.500000   \n",
       "75%        0.853553      0.853553      0.000000      0.516729      0.607287   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...            14            15            16            17  \\\n",
       "count  ...  49640.000000  49640.000000  49640.000000  49640.000000   \n",
       "mean   ...      0.495875      0.003891      0.422468      0.505024   \n",
       "std    ...      0.353346      0.029044      0.155201      0.160298   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.146447      0.000000      0.313433      0.395918   \n",
       "50%    ...      0.500000      0.000000      0.403448      0.497976   \n",
       "75%    ...      0.853553      0.000000      0.516854      0.605691   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 18            19            20            21            22  \\\n",
       "count  49640.000000  49640.000000  49640.000000  49640.000000  49640.000000   \n",
       "mean       0.746911      0.505727      0.471356      0.497698      0.496346   \n",
       "std        0.182331      0.132072      0.149600      0.354302      0.352784   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.638889      0.430387      0.353372      0.146447      0.146447   \n",
       "50%        0.782051      0.495806      0.459387      0.500000      0.500000   \n",
       "75%        0.888889      0.571992      0.572898      0.853553      0.853553   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 23  \n",
       "count  49640.000000  \n",
       "mean       0.003880  \n",
       "std        0.029017  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arquivos_pickle = ['A602','A621','A627','A636','A652']\n",
    "\n",
    "flag = True\n",
    "for arquivo in arquivos_pickle:\n",
    "    caminho = '../data/atmoseer/'\n",
    "    extensao = '.pickle'\n",
    "\n",
    "    outfilename = caminho + arquivo + extensao\n",
    "    print(outfilename)\n",
    "\n",
    "    f = open(outfilename, 'rb')\n",
    "\n",
    "    #Primeira ocorrencia do arquivo\n",
    "    if (flag):\n",
    "        (X_train , y_train , X_val , y_val , X_test , y_test ) = pickle.load(f) \n",
    "        flag = False\n",
    "    else:\n",
    "        (X_train_i , y_train_i , X_val_i , y_val_i , X_test_i , y_test_i ) = pickle.load(f) \n",
    "\n",
    "        # Adicionar conteúdo do arquivo pickle acumulando valores\n",
    "        X_train = np.vstack((X_train, X_train_i))\n",
    "        y_train = np.vstack((y_train, y_train_i))\n",
    "\n",
    "        X_val = np.vstack((X_val, X_val_i))\n",
    "        y_val = np.vstack((y_val, y_val_i))\n",
    "\n",
    "        X_test = np.vstack((X_test, X_test_i))\n",
    "        y_test = np.vstack((y_test, y_test_i))\n",
    "\n",
    "\n",
    "# Converter as matrizes para dataframes\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_validation = pd.DataFrame(X_val)\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_train.describe()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ae5ae4fd88e2bd38bf8e29a2e5dfd21f9bc40e4d52fc97fa5cc46517c6bfe74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
